# 第二章 信源及其熵2.1-2.3

## 事件的信息量
A是一个事件，P（A）是该时间发生的概率。
事件A信息量的定义：
$$ I(A)= -log_2(P(A))$$

 单位：比特

这个定义的意思是，当概率越小的事情发生，消除的不确定性越多，我们得到越多的信息量。

有时候也称为某个时间的信息熵

 ## 信息熵

### 信息熵的定义 
对于随机变量X，我们定义它的信息熵如下：

 $$H(X)=E(-log(p(x_i)))=\sum -log(p(x_i))p(x_i) $$

 它表示以这个随机变量为信源的信号，每次我们可以平均从中获得多少的信息

### 信息熵的性质 

1. 对称性 
2. 确定性
    H(X)反映的是信源总体的不确定性，当一个分量发生概率很大，确定性很小，熵值就很小
3. 非负性 
4. 扩展性 
   当信源的符号数量增多时，而这些符号发生的概率很小，信源的熵不变
5. 可加性 ，对于独立信源X,Y
   $$ H(X+Y)=H(X)+H(Y)$$

6.    **极值性** 
   当且仅当各个符号出现的概率相同时，信息熵取最大，也就是总体不确定最大，获得的平均信号最大。
   $$H(X)=log(n)$$

   n为符号个数
   



## 信号序列的信息量 &信息熵 
x1x2x3x4是一个离散序列

序列的信息量等于

$$I=\sum I(X=x_i)$$

序列的信息熵等于 （x均独立同分布）

$$H(NX)=4*H(X)$$



 